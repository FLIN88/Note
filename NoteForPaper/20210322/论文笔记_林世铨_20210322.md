<center>

# è®ºæ–‡ç¬”è®°
</center>


<center>

## **Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset**

</center>

---
* Carreira J, Zisserman A. Quo vadis, action recognition? a new model and the kinetics dataset [C] proceedings of the IEEE Conference on Computer Vision and Pattern Reco

### é˜…è¯»åŠ¨æœº
ç»å…¸çš„I3Dæ¨¡å‹çš„å‡ºå¤„ã€‚

### ç®€è¿°

æœ¬æ–‡å€Ÿé‰´ ImageNet çš„æ€æƒ³ï¼šå¤„ç†äºŒç»´å›¾åƒæ˜¯å¯ä»¥å°†ä¸€ä¸ªåœ¨å·¨å¤§çš„æ•°æ®é›†ä¸­é¢„è®­ç»ƒè¿‡çš„ä¼˜ç§€ä¸”é²æ£’æ¨¡å‹çš„æ¨¡å‹æ‹¿åˆ°è¾ƒå°çš„æ•°æ®é›†ä¸­è¿›è¡Œ fine-tuning å¯ä»¥è·å¾—ä¼˜äºç›´æ¥ä»å°æ•°æ®é›†ä¸­è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ï¼Œåœ¨å°æ•°æ®é›†ä¸­ç®€å•æ¨¡å‹æ€§èƒ½å¯èƒ½è¾¾ä¸åˆ°è¦æ±‚ï¼Œè€Œå¤æ‚æ¨¡å‹åˆ™æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å¸Œæœ›åœ¨å¯¹è§†é¢‘çš„å¤„ç†ä¸­ï¼ˆåŠ¨ä½œè¯†åˆ«ï¼‰ä¹Ÿæ„å»ºè¿™æ ·ä¸€ä¸ªå·¨å¤§çš„æ•°æ®é›†ä»¥åŠä¼˜ç§€ä¸”é²æ£’çš„æ¨¡å‹ï¼Œäºæ˜¯æå‡ºäº† I3D æ¨¡å‹å’Œ Kinetics æ•°æ®é›†ï¼Œå¹¶ä¸å½“æ—¶æµè¡Œçš„æ¯”è¾ƒä¼˜ç§€çš„æ¨¡å‹ä¹Ÿåœ¨ Kinetics æ•°æ®é›†ä¸Šè¿›è¡Œä¸è®­ç»ƒï¼Œç„¶ååˆ†åˆ«åœ¨ä¸¤ä¸ªå°çš„åŠ¨ä½œè¯†åˆ«æ•°æ®é›† HMDB-51 å’Œ UCF-101 ä¸Šè¿›è¡Œ fine-tuning åæµ‹è¯•å¹¶æ¯”è¾ƒï¼ŒæŠŠå®ƒä»¬ outperform æ‰ã€‚

### Abstact
The paucity of videos in current action classification
datasets (UCF-101 and HMDB-51) has made it difficult
to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video
dataset. Kinetics has two orders of magnitude more data,
with 400 human action classes and over 400 clips per
class, and is collected from realistic, challenging YouTube
videos. We provide an analysis on how current architectures
fare on the task of action classification on this dataset and
how much performance improves on the smaller benchmark
datasets after pre-training on Kinetics.
We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible
to learn seamless spatio-temporal feature extractors from
video while leveraging successful ImageNet architecture
designs and even their parameters. We show that, after
pre-training on Kinetics, I3D models considerably improve
upon the state-of-the-art in action classification, reaching
80.2% on HMDB-51 and 97.9% on UCF-101.

### åŠ¨ä½œè¯†åˆ«æ¨¡å‹
éšç€ 2D å·ç§¯æ¨¡å‹æ—¥æ¸æˆç†Ÿï¼Œç°æœ‰çš„è§†é¢‘å¤„ç†ä¸»è¦åˆ†åˆ«æ˜¯ 2D æ ¸å’Œ 3D æ ¸çš„åŒºåˆ«ï¼Œå…¶ä¸­ 2D æ ¸ ä¸€èˆ¬æœ‰ RNN çš„ LSTM ï¼Œä»¥åŠä¸€äº›åŒæµèåˆçš„æ¨¡å‹ï¼Œ3D æ ¸çš„æœ‰ C3D ã€‚å› ä¸º 3D å·ç§¯çš„å‚æ•°é‡å·¨å¤§ï¼Œå¹¶ä¸”å¯ç”¨æ•°æ®é›†ååˆ†æœ‰é™ï¼Œä»¥å‰çš„ 3D å·ç§¯ç»“æ„å±‚æ•°ååˆ†æµ…ï¼ˆ C3D åªæœ‰ 8 å±‚ï¼‰ã€‚è§‚å¯Ÿåˆ°æ·±åº¦ç½‘ç»œç»“æ„ï¼ˆ Inception, VGG-16, ResNet ï¼‰éƒ½å¯ä»¥è†¨èƒ€ï¼ˆinflateï¼‰æ¥åšæ—¶ç©ºç‰¹å¾æå–ã€‚

ä»¥ä¸‹ç”¨ä¼ ç»Ÿçš„æ–¹å¼æ„å»º 3 æ—¶ç©ºç‰¹å¾æå–çš„æ¨¡å‹ï¼Œå†æœ¬æ–‡æå‡ºçš„ I3D æ¨¡å‹ã€‚ä¸ºäº†å¯æ¯”æ€§ï¼Œå¤§å®¶éƒ½ç”¨å¸¦æœ‰ BN çš„ Inception-V1 ä½œä¸º back boneã€‚

![img](./1.png)

#### 1. ConvNet + LSTM

åœ¨å¸¦æœ‰ BN çš„ Inception-V1 æœ€åçš„ average pooling å±‚åŠ ä¸€ä¸ªå±‚ $512$ ä¸ªèŠ‚ç‚¹çš„éšå±‚ï¼Œå†åŠ  $1$ ä¸ªå…¨è¿æ¥è¾“å‡ºå±‚åšç»´åˆ†ç±»

#### 2. 3D ConvNets

3D ConvNets çš„ç¼ºç‚¹æ˜¯ä¸èƒ½ä½¿ç”¨ ImageNet è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™é‡Œä½¿ç”¨ä¸€ä¸ªæ ¹æ® C3D ä¿®æ”¹çš„æ¨¡å‹ï¼Œ$8$ ä¸ªå·ç§¯å±‚ï¼Œ$5$ ä¸ªæ± åŒ–å±‚ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ª $16$ å¸§çš„ $112Ã—122$ çš„çŸ­ç‰‡ï¼Œä¸ C3D ä¸åŒçš„æ˜¯åœ¨æ‰€æœ‰çš„å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚åéƒ½åŠ ä¸Š BN ï¼Œå¦ä¸€ä¸ªä¸åŒä¹‹å¤„æ˜¯æŠŠç¬¬ä¸€ä¸ªæ± åŒ–å±‚çš„ stride ä» $1$ æ”¹æˆ $2$ ï¼Œè¿™æ ·èŠ‚çœå†…å­˜æ¥ä½¿ç”¨æ›´å¤§çš„ batch 

#### 3. Two-Stream Networks

ç”¨ Inception-V1 è¾“å…¥ç›¸éš” $10$ å¸§é‡‡æ ·çš„ $5$ å¸§ RGB å›¾åƒï¼Œå¯¹åº”çš„å…‰æµæ•°æ®ä¸€èµ·è¾“å…¥æ¨¡å‹ä¸­ï¼Œåœ¨æœ€åä¸€ä¸ª average pooling å‰çš„ spatial å’Œ motion è¦ç»è¿‡ä¸€ä¸ª $3Ã—3Ã—3$ çš„å·ç§¯ï¼Œè¾“å‡º $512$ ä¸ªé€šé“ï¼Œç„¶åå†ç»è¿‡ä¸€ä¸ª $3Ã—3Ã—3$ çš„ max-poolingå±‚å’Œä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚æ‰€æœ‰æ–°å±‚çš„æƒå€¼éƒ½ç”¨ Gaussian noise åˆå§‹åŒ–ã€‚

#### 4. Two-Stream Inflated 3D ConvNets

* Inflating 2D ConvNets into 3D

    æŠŠ 2D çš„å·ç§¯æ ¸æ‰©å±•ä¸º 3D çš„ï¼Œæ¯”å¦‚ $NÃ—N$ å˜æˆ $NÃ—NÃ—N$ ï¼Œå¢åŠ æ—¶é—´ä¸Šçš„ç»´åº¦

* Bootstrapping 3D filters from 2D Filters

    è€ƒè™‘å¦‚æœæŠŠä¸€å¼  2D å›¾åƒé€šè¿‡å¤åˆ¶å¾ˆå¤šå¸§æˆä¸ºä¸€æ®µè§†é¢‘ï¼Œç”¨ $NÃ—NÃ—N$ çš„å·ç§¯æ ¸è¿›è¡Œå·ç§¯å¾—åˆ°çš„å€¼åº”è¯¥ä¸ $NÃ—N$ çš„ 2D å·ç§¯å¾—åˆ°çš„å€¼ä¸€æ ·ï¼Œé‚£ä¹ˆå¯ä»¥æŠŠ 2D å·ç§¯æ ¸å¤åˆ¶ $N$ ä»½ï¼Œç„¶åæŠŠæ‰€æœ‰çš„å€¼é™¤ä»¥ $N$

* Pacing receptive field growth in space, time and network depth

    ç‰¹å¾çš„æ„Ÿå—ä¹Ÿåœ¨ç½‘ç»œä¸­å¤„äºè¶Šæ·±çš„ä½ç½®ï¼Œå…¶æ„Ÿå—é‡å°±è¶Šå®½ï¼Œé€šå¸¸éƒ½æ˜¯å¯¹é•¿å®½ä¸¤ä¸ªç»´åº¦æ˜¯è¿›è¡ŒåŒç­‰å¤„ç†çš„ï¼Œä½†åœ¨è¿™é‡Œï¼Œåœ¨æ—¶é—´ç»´åº¦ä¸Šå¹¶ä¸èƒ½è·Ÿé•¿å®½è¿›è¡ŒåŒç­‰å¤„ç†ï¼Œå› ä¸ºåŒæ ·åœ¨ä¸åŒçš„å¸§ç‡ä¸‹ï¼ŒåŒæ ·çš„å¸§æ•°å¯¹åº”çš„æ—¶é—´å¹¶ä¸ç›¸åŒï¼Œå› æ­¤ï¼Œè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„è§†é¢‘æ¥å…·ä½“è€ƒè™‘


æ ¹æ® Inception-V1 æ”¹è¿›çš„ I3D æ¨¡å‹å¦‚ä¸‹å›¾æ‰€ç¤º
![3](3.png)
### å®éªŒå¯¹æ¯”

![2](2.png)

ä½œè€…åœ¨å‰æ–‡æåˆ°çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸‰ç§å®éªŒï¼Œé¦–å…ˆåœ¨ Kinetics ä¸ŠæŠŠæ¨¡å‹è¿›è¡Œä¸è®­ç»ƒ

1. ç›´æ¥åœ¨å…¶ä»–ä¸¤ä¸ªæ•°æ®é›†çš„æµ‹è¯•é›†ä¸Šä½¿ç”¨ï¼ˆOriginalï¼‰

2. å›ºå®šç½‘ç»œå‚æ•°ï¼Œåœ¨ç”¨å…¶ä»–ä¸¤ä¸ªæ•°æ®é›†çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒåˆ†ç±»å™¨åå†ç”¨æµ‹è¯•é›†æµ‹è¯•(Fixed)

3. åœ¨å…¶ä»–ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œ fine-turn ç½‘ç»œä»¥åŠè®­ç»ƒåˆ†ç±»å™¨(Full-FT)

å¯è§æ— è®ºæ˜¯ä½•ç§æµ‹è¯•æ–¹å¼ï¼ŒI3D éƒ½ outperform å…¶ä»–çš„æ¨¡å‹ã€‚

* åˆ«çš„æ¨¡å‹åœ¨å…¶ä»–ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½æ˜¯å…‰æµçš„è´¡çŒ®æ›´å¤§ï¼Œè€Œ Kinestics ä¸Šåˆ™æ˜¯ RGB çš„è´¡çŒ®æ›´å¤§ï¼Œè¯´æ˜ Kinetics æ•°æ®é›†ä¸Šå›¾åƒåŠ¨ä½œæ›´æ˜æ˜¾äº›ï¼Œä¹Ÿæ›´è¯´æ˜ I3D æ‹¥æœ‰å¾ˆå¼ºçš„å›¾åƒä¸ŠåŠ¨ä½œçš„æ•æ‰èƒ½åŠ›

* ä» MiniKinetics åˆ° Kinetics çš„æå‡è¯´æ˜ 3D ConvNet éœ€è¦å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒå‡ºé²æ£’çš„åŠ¨ä½œæ•æ‰èƒ½åŠ›

<center>

## **Deep Reinforcement Learning with Double Q-learning**

</center>

---

* Van Hasselt, Hado, Arthur Guez, and David Silver. "Deep reinforcement learning with double q-learning." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30. No. 1. 2016.

### é˜…è¯»åŠ¨æœº

Double Q-learning æ˜¯ä¸€ä¸ªå¯¹ Q-learning å­¦ä¹ è´¨é‡çš„é‡è¦æ”¹è¿›ã€‚

### ç®€è¿°

å½“æ—¶ç°æœ‰çš„ Q-learning çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éƒ½æ™®éå­˜åœ¨å¯¹ action-value çš„ overestimate é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡ç†è®ºæ¨å¯¼åŠ ä¸Šå®ä¾‹è¯æ˜ overestimate é—®é¢˜ä¸ä»…ä¼šå¯¼è‡´ action-value çš„è¿‡é«˜ä¼°è®¡ï¼Œè¿˜ä¼šé˜»ç¢å¯¹æœ€å³ç­–ç•¥çš„å­¦ä¹ ï¼Œé™åˆ¶æ¨¡å‹çš„è¡¨ç°ï¼Œéšåæœ¬æ–‡æå‡º Double Q-learning æ¥è§£å†³äº† Overestimate é—®é¢˜ï¼Œå®é™…ä¸Šå°±æ˜¯æŠŠ Q-learning ä¸­ target value çš„è®¡ç®—ä¸­çš„åŠ¨ä½œé€‰æ‹©å’Œä»·å€¼ä¼°è®¡è¿›è¡Œè§£è€¦ï¼Œè¿™ä¸ªæ”¹è¿›è§£å†³äº† overestimate é—®é¢˜ï¼Œå¹¶åœ¨æå‡ DQN åœ¨ Atari æ¸¸æˆä¸Šçš„è¡¨ç°ã€‚

### Abstract

The popular Q-learning algorithm is known to overestimate
action values under certain conditions. It was not previously
known whether, in practice, such overestimations are common, whether they harm performance, and whether they can
generally be prevented. In this paper, we answer all these
questions affirmatively. In particular, we first show that the
recent DQN algorithm, which combines Q-learning with a
deep neural network, suffers from substantial overestimations
in some games in the Atari 2600 domain. We then show that
the idea behind the Double Q-learning algorithm, which was
introduced in a tabular setting, can be generalized to work
with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better
performance on several games.

### Double Q-learning

ä¼ ç»Ÿçš„ Q-learning çš„ target è®¡ç®—æ–¹å¼å¦‚ä¸‹

![5](5.png)

ä»·å€¼ä¼°è®¡å’ŒåŠ¨ä½œé€‰æ‹©åŒçš„æ˜¯åŒä¸€ä¸ª $Q$ å¾ˆå®¹æ˜“å¯¼è‡´è¿‡åº¦ä¹è§‚é—®é¢˜ã€‚

ä¼ ç»Ÿçš„ Q-learning ä¸­åªç”¨ä¸€ä¸ªè¡¨è¿›è¡Œä»·å€¼ä¼°è®¡å’ŒåŠ¨ä½œé€‰æ‹©ï¼Œå¦‚æœæŠŠä»·å€¼ä¼°è®¡å’ŒåŠ¨ä½œé€‰æ‹©åˆ†ç¦»å¼€ï¼Œå³ä¸¤ä¸ªåŒæ ·çš„ $Q$ ï¼Œæ¯æ¬¡å­¦ä¹ ç­‰æ¦‚ç‡çš„æŠŠä»»åŠ¡åˆ†é…ç»™å…¶ä¸­ä¸€ä¸ªï¼Œå¹¶åªåœ¨ä»·å€¼ä¼°è®¡çš„ $Q$ ä¸Šè¿›è¡Œå­¦ä¹ ï¼Œå°±æå¤§çš„ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

æ–°çš„ target è®¡ç®—æ–¹å¼å¦‚ä¸‹

![6](6.png)

### ä¼°è®¡è¯¯å·®é€ æˆçš„è¿‡åº¦ä¹è§‚

ä»¥ä¸‹é€šè¿‡è¯¥å¼•ç†è¯æ˜äº†åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ— è®ºä»€ä¹ˆå™ªå£°éƒ½ä¼šå¯¼è‡´ä¼ ç»Ÿ Q-learning å‡ºç°è¿‡åº¦ä¹è§‚é—®é¢˜ï¼Œå¹¶ç»™å‡ºäº†è¯¯å·®ä¸‹ç•Œã€‚å®é™…å°±æ˜¯å³ä½¿å‡å€¼æ˜¯æ— åå·®çš„ï¼Œé—®é¢˜ä¾ç„¶å­˜åœ¨ï¼Œå› ä¸ºæœ‰äº† MAX æ“ä½œã€‚

![4](4.png)


ä»¥ä¸‹é€šè¿‡ä¸€ä¸ªå®ä¾‹å±•ç°äº†è¿‡åº¦ä¹è§‚çš„å½¢æˆ

![7](7.png)

ç¬¬ä¸€è¡Œï¼šç”¨ $6$ é˜¶å¯¹ $sin(s)$ è¿›è¡Œè¿‘ä¼¼

ç¬¬äºŒè¡Œï¼šç”¨ $6$ é˜¶å¯¹ $2^{-s^2}$ è¿›è¡Œè¿‘ä¼¼

ç¬¬ä¸‰è¡Œï¼šç”¨ $9$ é˜¶å¯¹ $2^{-s^2}$ è¿›è¡Œè¿‘ä¼¼

ä¸­é—´ä¸€åˆ—æ˜¯ä¸åŒçš„é‡‡æ ·ç‚¹è¿›è¡Œå¤šä¸ªè¿‘ä¼¼

å³è¾¹ä¸€åˆ—æ˜¯å¯¹å¤šä¸ªè¿‘ä¼¼å– MAX ï¼Œå¯¹æ¯”åŸå‡½æ•°å¯ä»¥çœ‹å‡ºå¯¹è¿‡åº¦ä¹è§‚æ•ˆåº”

### Double DQN

DQN æœ¬èº«å°±å­˜åœ¨ä¸¤ä¸ª $Q$ ç½‘ç»œï¼Œå› æ­¤åªéœ€è¿›è¡Œå¾ˆå°çš„æ”¹åŠ¨å°±å¯ä»¥å®ç° Double DQN ï¼Œå³æŠŠ target çš„è®¡ç®—å˜æˆå¦‚ä¸‹

![8](8.png)

å®é™…å°±æ˜¯ç”¨ æœ€æ–°çš„ç½‘ç»œæ¥è¿›è¡ŒåŠ¨ä½œé€‰æ‹©ï¼Œç”¨æ—§ç½‘ç»œè¿›è¡Œä»·å€¼ä¼°è®¡ï¼ŒåŸæœ¬æ˜¯äºŒè€…éƒ½åœ¨æ—§ç½‘ç»œä¸Šè¿›è¡Œã€‚

è¿™ä¾¿å®ç°æ¥äº†ä¸æ”¹å˜æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå®ç°äº† Double DQN

### å®éªŒæ€»ç»“

æœ¬æ–‡é€šè¿‡å®éªŒç»“æœæ€»ç»“å‡º overestimate ä¸ä½†é²æ£’æ€§ä¸å¼ºï¼Œä¸ç¨³å®šæ€§è¿˜å¯èƒ½å¯¼è‡´å­¦ä¹ åˆ°ä¸å¥½çš„ç­–ç•¥ï¼ŒDouble DQN æå¤§çš„å‡å°‘äº† overestimate æ•ˆåº”ï¼Œä½¿å¾—å­¦ä¹ æ›²çº¿æ›´åŠ çš„ç¨³å®šï¼Œå¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œè¿˜å¯ä»¥å­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ï¼Œå¹¶ä¸”è·å¾—æ›´å¥½çš„è¡¨ç°ï¼
<center>

## **PRIORITIZED EXPERIENCE REPLAY**

</center>

---
* Schaul, Tom, et al. "Prioritized experience replay." arXiv preprint arXiv:1511.05952 (2015).

### é˜…è¯»åŠ¨æœº

é’ˆå¯¹å¼ºåŒ–å­¦ä¹ çš„ sample æ–¹æ³•çš„ä¸€ä¸ªé‡è¦çš„ä¼˜åŒ–

### ç®€è¿°

ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä¸­è¿ç”¨ experience replay æ¥è§£é™¤ transition é—´çš„ç›¸å…³æ€§ï¼ŒæŠŠæœ€è¿‘çš„ä¸€äº› transition å­˜åˆ°ä¸€ä¸ª replay buffer é‡Œï¼Œç„¶åå†åœ¨é‡Œé¢éšæœºæŠ½å–ä¸€äº›ç»„æˆ batch æ¥å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œè¿™é‡Œæ²¡æœ‰è€ƒè™‘åˆ°è¿™äº› transition çš„è´¨é‡ï¼Œå¯èƒ½éƒ¨åˆ† transition å¯¹æ¨¡å‹çš„ä¼˜åŒ–å¹¶æœ‰å¤ªå¤§çš„ä½œç”¨ï¼Œå°±æ˜¯æœ‰äº›å¯¹ç­–ç•¥ä¼˜åŒ–ä½œç”¨ä¸å¤§çš„ç»éªŒå æ®äº† replay buffer ã€‚æœ¬æ–‡æå‡ºäº†ç»™ transition å®‰ä¸Šä¼˜å…ˆçº§ï¼Œä¼˜å…ˆå­¦ä¹ é‚£äº›å¯¹ç­–ç•¥ä¼˜åŒ–æ›´æœ‰ç”¨çš„ transition ï¼Œä»¥æ­¤æå¤§æé«˜äº†å­¦ä¹ çš„é€Ÿåº¦ï¼Œè¿˜æœ‰æ”¹å–„å­¦ä¹ çš„ç­–ç•¥è´¨é‡ã€‚

### Abstract

Experience replay lets online reinforcement learning agents remember and reuse
experiences from the past. In prior work, experience transitions were uniformly
sampled from a replay memory. However, this approach simply replays transitions
at the same frequency that they were originally experienced, regardless of their
significance. In this paper we develop a framework for prioritizing experience,
so as to replay important transitions more frequently, and therefore learn more
efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a
reinforcement learning algorithm that achieved human-level performance across
many Atari games. DQN with prioritized experience replay achieves a new stateof-the-art, outperforming DQN with uniform replay on 42 out of 57 games.

###  temporal-difference (TD) error

TD error  å°±æ˜¯ DQN ä¸­çš„ $|\delta|$ è®¡ç®—å¦‚ä¸‹

![9](9.png)

ä¼˜å…ˆçº§å°±æ˜¯æ ¹æ® $|\delta|$ è¿›è¡Œè®¾ç½®çš„

### PRIORITIZING WITH TD-ERROR

* å¯¹æ¯ä¸ª transition è®°å½•æœ€è¿‘ä¸€æ¬¡è¿›è¡Œ replay æ—¶çš„ TD error
* æ¯æ¬¡å–å…·æœ‰æœ€å¤§çš„ TD error çš„ transiton è¿›è¡Œ replay
* å¯¹äºæ–°æ¥çš„ transition è®¾ç½®ä¸ºæœ€é«˜ä¼˜å…ˆçº§ï¼Œä»¥ä¾¿äºå¾—åˆ°å®ƒçš„ TD error

### éšæœºä¼˜å…ˆ

ä¼˜å…ˆçº§çš„è®¾ç½®å¾€å¾€éƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œå…¶ä¸­çš„ä¸€ä¸ªå°±æ˜¯ä½ä¼˜å…ˆçº§çš„ transition å¾ˆé•¿æ—¶é—´å¾—ä¸åˆ°æˆ–è€…æ°¸è¿œå¾—ä¸åˆ° replay ã€‚è¿˜æœ‰å°±æ˜¯è¿™å¯¼è‡´æ¨¡å‹å¯¹å™ªå£°çš„æ•æ„Ÿæ€§å¢åŠ ã€‚åŒæ—¶è¿˜å¯¼è‡´æ ·æœ¬ç¼ºå°‘å¤šæ ·æ€§ï¼Œå­¦ä¹ ä¸åˆ°çœŸå®çš„åˆ†å¸ƒï¼Œé€ æˆè¿‡æ‹Ÿåˆã€‚

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ä¸­æå‡ºäº†éšæœºä¼˜å…ˆçš„é‡‡æ ·ç­–ç•¥ï¼Œè®©é‡‡æ ·ç­–ç•¥ä»‹äºéšæœºé‡‡æ ·å’Œè´ªå¿ƒé‡‡æ ·ä¹‹é—´ã€‚æœ¬æ–‡æå‡ºä¸¤ç§éšæœºä¼˜å…ˆçš„æ–¹æ³•

#### proportional prioritization

![10](10.png)

ä¸Šå›¾æ˜¯æ¯ä¸ª transition è¢«é‡‡æ ·çš„æ¦‚ç‡ï¼Œå…¶ä¸­ $p_i = |\delta| + \epsilon$ æ˜¯æ¯ä¸ª tansition çš„ä¼˜å…ˆçº§ï¼Œ$\alpha$ æ˜¯æ§åˆ¶ä¼˜å…ˆçº§çš„ä½¿ç”¨ç¨‹åº¦ï¼Œå¦‚æœ $\alpha = 0$ å°±ç­‰åŒäºä¸€èˆ¬çš„éšæœºé‡‡æ ·ã€‚

#### rank-based prioritization

$p_i = \frac{1}{rank(i)}$


ä»¥ä¸‹ä½¿ç”¨ prioritized replay çš„ DQN çš„ç®—æ³•

![11](11.png)


### å‡é€€åå·®

éšæœºæ›´æ–°æ˜¯å…³é”®æ˜¯ batch çš„åˆ†å¸ƒè·ŸçœŸå®æ•°æ®çš„åˆ†å¸ƒç›¸åŒï¼Œä¼˜å…ˆçº§ç”±äºæ‰“ç ´äº†è¿™ç§åˆ†å¸ƒï¼Œå¼•å…¥äº†åå·®ï¼Œè¿™å¯èƒ½æ”¹å˜æœ€åæ”¶æ•›çš„ solution ã€‚è¿™æ˜¯ä¸€ä¸ªéœ€è¦è§£å†³çš„é—®é¢˜ã€‚


è§£å†³çš„æ–¹æ³•æ˜¯æ›´æ–°ç½‘ç»œæ˜¯ç”¨çš„ $\delta$ è¯¥ä¸º $\omega_i\delta_i$ å¹¶é€šè¿‡æ§åˆ¶ $\beta$ æ¥æ§åˆ¶ $\omega$ ï¼Œ $\omega$ çš„å®šä¹‰å¦‚ä¸‹ï¼Œå½“ $\beta = 1$ æ—¶ä¼˜å…ˆçº§å°±æ²¡æœ‰äº†ä½œç”¨ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸º Importance-sampling ã€‚

![12](12.png)

åŒæ—¶å¢åŠ  $\alpha$ å’Œ $\beta$ å¯ä»¥è®©ä¼˜å…ˆé€‰æ‹©æ•ˆæœé€æ¸å¢åŠ è€Œåˆé€æ¸å‡å¼±æ›´æ–°åŠ›åº¦ã€‚

åŒæ¢¯åº¦ä¸‹é™æ³•ç›¸ä¼¼ï¼Œå½“æ­¥é•¿è¶³å¤ŸçŸ­æ—¶ï¼ŒåŠæ—¶ä¼°è®¡å­˜åœ¨ä¸€é˜¶åå·®ï¼Œæœ€åä¹Ÿå¯ä»¥æ”¶æ•›åˆ°æ­£ç¡®çš„æœ€å€¼ã€‚å› æ­¤è®­ç»ƒæ—¶çš„åšæ³•æ˜¯æŠŠ $\alpha$ å’Œ $\beta$ åŒæ—¶é€æ¸è¶‹è¿‘äº $1$ ã€‚è¿™æ ·åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼˜å…ˆé€‰æ‹©æ•ˆæœä¸æ–­æå‡ï¼Œè€Œæ­¥é•¿é€æ¸ç¼©å°ï¼Œå¯ä»¥æœ‰æ•ˆçš„æŠ‘åˆ¶åå·®çš„å‡ºç°ï¼Œåœ¨æœ€åæ”¶æ•›åˆ°æ— åå·®çš„ solutionã€‚

### æ‰©å±•

ä¼˜å…ˆçš„æ¦‚å¿µå¯ä»¥æ‰©å±•åˆ°å¼ºåŒ–å­¦ä¹ ä»¥å¤–çš„èŒƒå›´ï¼Œæ¯”å¦‚ç›‘ç£å­¦ä¹ ä¸­å¯ä»¥å¯¹æ ·æœ¬è¿›è¡Œä¼˜å…ˆçº§å¤„ç†ï¼ŒåŠ å¿«å­¦ä¹ é€Ÿç‡ï¼ŒåŒæ—¶åœ¨æ ·æœ¬ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œå°‘å¾—ä¸€ç±»è¿˜ä¼šå› ä¸ºå­¦ä¹ ä¸å¤Ÿè€Œäº§ç”Ÿè¾ƒé«˜çš„ä¼˜å…ˆçº§ï¼ŒæŒ‡å¯¼æ¨¡å‹æ›´å¤šå»å­¦ä¹ ï¼Œç¼“è§£æ ·æœ¬ä¸å¹³è¡¡å¸¦æ¥çš„åå·®ã€‚

é™¤äº†ä¼˜å…ˆé‡‡æ ·å¤–ï¼Œä¼˜å…ˆçº§çš„æ¦‚å¿µè¿˜å¯ä»¥ç”¨äº replay buffer çš„ç®¡ç†ï¼Œå³æŠŠå¯¹å“ªäº›æ ·æœ¬è¿›è¡Œé‡‡æ ·æ‰©å±•åˆ°å¯¹å“ªäº›æ ·æœ¬è¿›è¡Œå­˜å‚¨ï¼ŒæŠŠæ²¡æœ‰ä»€ä¹ˆå­¦ä¹ ä»·å€¼çš„æ ·æœ¬ä¸¢å¼ƒï¼Œåªå­˜å‚¨é‚£äº›å­¦ä¹ ä»·å€¼é«˜çš„ï¼Œè¿™æ ·å¯ä»¥èŠ‚çœå†…å­˜çš„ä½¿ç”¨ã€‚

<center>

## **Noisy Networks for Exploration**

</center>


---
* Fortunato, Meire, et al. "Noisy networks for exploration." arXiv preprint arXiv:1706.10295 (2017).
  
### é˜…è¯»åŠ¨æœº

å¯¹å¼ºåŒ–å­¦ä¹ æœç´¢ç­–ç•¥çš„ä¸€ä¸ªé‡è¦ä¼˜åŒ–

### ç®€è¿°

å¼ºåŒ–å­¦ä¹ ä¸­å¯¹ç¯å¢ƒçš„æ¢ç´¢æ˜¯å­¦ä¹ çš„å‰æï¼Œå½“æ—¶ç°æœ‰çš„æ¢ç´¢ç­–ç•¥å¤§å¤šéƒ½æ˜¯éšæœºæ¢ç´¢ï¼Œå¹¶ä¸”äººå·¥è¿›è¡Œè°ƒå‚ï¼Œå¦‚ $\epsilon$-greedy è¿™äº›éšæœºç®—æ³•ï¼Œæ²¡æœ‰ä¸€ç§æ™®é€‚çš„å¯å­¦ä¹ çš„ä¼˜ç§€æ¢ç´¢ç­–ç•¥ï¼Œæœ¬æ–‡å°±æ­¤è¿›è¡Œç ”ç©¶ï¼Œæå‡ºäº†åœ¨ç½‘ç»œä¸­åŠ å™ªå£°çš„æ–¹å¼çš„ NoisyNet æ¨¡å‹ã€‚

### Abstract

We introduce NoisyNet, a deep reinforcement learning agent with parametric noise
added to its weights, and show that the induced stochasticity of the agentâ€™s policy
can be used to aid efficient exploration. The parameters of the noise are learned
with gradient descent along with the remaining network weights. NoisyNet is
straightforward to implement and adds little computational overhead. We find that
replacing the conventional exploration heuristics for A3C, DQN and dueling agents
(entropy reward and -greedy respectively) with NoisyNet yields substantially
higher scores for a wide range of Atari games, in some cases advancing the agent
from sub to super-human performance.

### NoisyNets for Reinforcement Learning

è®¾ $y = f_{\theta}(x),\theta = \mu + \sigma \odot \epsilon , \zeta = (\mu, \sigma)$ 

æŠŠ $y = \omega x + b$ æ”¹ä¸º $y = (\mu^{\omega} + \sigma^{\omega} \odot\epsilon^{\omega})x + (\mu^b + \sigma^b \odot\epsilon^b)$

å…¶ä¸­ $\mu , \sigma$ æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œ$\epsilon^{\omega}, \epsilon^b$ ,æ˜¯ zero-mean çš„å‘é‡å’ŒçŸ©é˜µã€‚

$\epsilon$ çš„æ„é€ æ–¹æ³•æœ‰ä¸¤ç§

å¯¹äº$p$ ç»´çš„è¾“å…¥ï¼Œ $q$ ç»´çš„è¾“å…¥

* æ¯ä¸ª $\epsilon_{i,j}$ ç‹¬ç«‹æ¥è‡ªä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œè¿™æ ·æœ‰

* åˆ†è§£é«˜æ–¯å™ªå£°ï¼Œå¯¹äºè¾“å…¥æœ‰ä¸€ä¸ª $p$ ä¸ª $\epsilon_i$ , è¾“å‡ºæœ‰ $q$ ä¸ª $\epsilon_j$ ï¼Œå– $\epsilon^{\omega}_{i,j} =f(\epsilon_i)f(\epsilon_j)$ ï¼Œ$\epsilon^b_j = f(\epsilon^b_j)$ ï¼Œå…¶ä¸­ $f$ ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œæœ¬æ–‡ä¸­ $f=sgn(x) \sqrt{|x|}$

æŸå¤±å‡½æ•°å¦‚ä¸‹

![13](13.png)

å³å¯¹ç½‘ç»œçš„è¾“å‡ºçš„æœŸæœ›ï¼Œè¿ç”¨è’™ç‰¹å¡æ´›æ¥è¿‘ä¼¼

![14](14.png)


### NosiyNet-DQN

![15](15.png)

å…¶ç®—æ³•å¦‚ä¸‹

![16](16.png)

